
\section{Search Guided Graph Neural Network}

We propose a graph neural network that adopts the same colouring mechanisms as BFC and DFC for feature propagation. Let $h_u^{(l)}$ be an embedding of vertex $u$ after the $l$-th layer. Our search-guided graph neural network propagates embeddings over a graph via the following update rule:
\begin{equation}
\label{eqn:gnn_layer}
  h_u^{(l+1)} = \text{MLP}\left(\left(1 + \epsilon^{(l+1)}\right)\cdot h_u^{(l)} \parallel
                        \sum_{v\in N_{\delta}(u)} h_{u\leftarrow v}^{(l+1)}
  \right)
\end{equation}
where
\begin{equation}
\label{eqn:gnn_huv}
  h_{u\leftarrow v}^{(l+1)} = \left(h_u^{(l)} +
                        \sum_{
                        w \in \eta_v(u)
                        } h_{w\leftarrow v}^{(l)}
  \right)W_{c}
\end{equation}
where, $h^{(0)}_u$ is the input vertex feature of $u$ and $\parallel$ is the concatenation operator. $h^{(l)}_{w\leftarrow v}$ is the vertex representation of $w$ obtained from a graph search rooted at $v$. When $w=v$, we have $h^{(l)}_{v\leftarrow v} = h^{(l)}_v$. Here, $W_{c}\in \mathbb{R}^{F\times H}$ is a learnable parameter matrix, where $F$ and $H$ are the dimensions of the input feature and hidden layer, respectively. Note that we use the same $W_c$ for all vertices in $\{u\in V : c=d(u,v)\}$, which have the same shortest path distance $c=d(u,v)$ to vertex $v$. For brevity, we use $\eta_v(u)$ to denote $\eta(u,E^{T_v}_{\text{tree}},E^{T_v}_{\text{back}})$.

For the model to learn an injective transformation, we use a multilayer perceptron (MLP) and a learnable scalar parameter $\epsilon$ adopted from \citet{xu2018powerful}.
The model architecture closely matches the one in Equations \ref{eqn:lvc-all} and \ref{eqn:lvc}, except that $\psi(\cdot)$ is replaced with a summation, $\phi(\cdot)$ is replaced by matrix multiplication, and MLP is used in place for $\rho(\cdot)$. 

% We can further simplify Equation \ref{eqn:gnn_huv} as 
% \begin{equation}
% \label{eqn:gnn_huv_simplified}
%   h_{u\leftarrow v}^{(l+1)} = \sum_{
%                         w \in \bar{N}_{v}(u)
%                         } h_{w\leftarrow v}^{(l)}
%   W_{c}
% \end{equation}\q{(I haven't gone through the proofs in the supplementary material yet - so not sure whether this would affect the expressivity)}
% where
% \begin{equation}
% \label{eqn:M_u_v}
%   \bar{N}_{v}(u) =  \cup_{
%   w \in \eta_v(u)
%   } \bar{N}_{v}(w) \cup \eta_v(u) \cup \{u\}
% \end{equation}
% % Equations \ref{eqn:gnn_huv_simplified} and \ref{eqn:M_u_v} are equivalent of replacing $\phi$ with a union and $\psi$ with an identity function in Equation \ref{eqn:lvc}. Both union and identity functions are injective. 
% $\bar{N}_{v}(u)$ is defined recursively and $\bar{N}_{v}(v) = \{v\}$.
% The simplified version does not involve vertex representations (or vertex colours) and can thus be pre-computed. 

We term the model as \textit{Search-guided Graph Neural Network}~(SGN). When $\eta_{\text{bfc}}(\cdot)$ is used in place for $\eta_v(\cdot)$, we call it SGN-BF. When $\eta_{\text{dfc}}(\cdot)$ is used, we call it SGN-DF.

\begin{restatable}[]{thm}{lvcgnn}
\label{thm:lvcgnn}
\model{} defined by the update rule in Equations \ref{eqn:gnn_layer} and \ref{eqn:gnn_huv} and with sufficiently many layers is as powerful as $LVC$.
\end{restatable}

\begin{table}[b]
    \centering
    \caption{Time and space complexity comparison.}
    \label{tab:complexity}
    \resizebox{0.9\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
         & MPNN & Graphormer-GD & SGN-BF & SGN-DF \\
         \midrule
         Time & $|V|+|E|$ & $|V|^2$ & $|V|+|V|d^{\delta-1}$ & $|V|+|V|d^{2\delta}$ \\
         Space & $|V|$ & $|V|$ & $|V|d^{\delta-1}$ & $|V|d^{2\delta}$ \\
         \bottomrule
    \end{tabular}
    }
\end{table}


\paragraph{Choice of $\delta$.} As of \cref{thm:bfcexpressitybeyond}, a larger $\delta$ implies higher expressivity for \model{}-BF in distinguishing isomorphic graphs. However, increasing $\delta$ also increases the size of $\eta_v(u)$, which  makes it more expensive to compute as more aggregation operations are needed. Further, a larger $\delta$ means a larger receptive field at each layer, which is more likely to cause \textit{over-squashing}~\citep{topping22oversquashing} leading to degrade performance on vertex-level tasks, e.g. vertex classification on heterophilic graphs. 

For \model{}-DF, increasing $\delta$ does not necessarily increase expressivity (\cref{thm:dfcexpressitynotbeyond}). However, having a larger $\delta$ means that the model can detect larger biconnected components. Therefore, in practice, we combine vertex representations from $\delta=1$ with a larger $\delta$ for \model{}-DF. This guarantees that the model is more expressive than 1-WL and allows the additional $\delta$ to be fine-tuned for each dataset and task.




\paragraph{Complexity.}
$\eta_v(u)$ in \autoref{eqn:gnn_huv} only needs to be computed once, along with graph searching. Therefore, the time complexity to compute $\eta_v(u)$ is on par with the adopted graph searching. Both BFS and DFS have the worst-case complexity $O(|V|+|E|)$ for searching the whole graph. Assuming $d$ is the average vertex degree, when the search is limited to $N_{\delta}(v)$, we have the complexity $O({d}^{\delta}(1+{d}))$. We compute $\eta_v(u)$ for each $v\in V$ so in total it is $O(|V|{d}^{\delta}(1+{d}))$. Each layer in \model{} (\autoref{eqn:gnn_layer}) aggregates $|N_{\delta}(u)|$ vertex representation vectors for each vertex. Each vertex representation further aggregates $|\eta_v(u)|$ vector in \autoref{eqn:gnn_huv}, i.e., $|N_{\delta}(u)|\cdot|\eta_v(u)|$ operations, or, $O({d}^{\delta}|\eta_v(u)|)$ in total. The magnitude of $|\eta_v(u)|$ can be very different for BFS and DFS, and varies from graph to graph. In the worst case of BFS where every vertex of hop $\delta+1$ is connected to every vertex of hop $\delta$; thus $|\eta_v(u)| = \frac{1 - {d}^{\delta}}{1-{d}}$. In the worst case of DFS, $\eta_v(u)$ includes all vertices in $N_{\delta}(u)$ which has the size of ${d}^{\delta}$. We compare the time and space complexity of our model in \cref{tab:complexity} with MPNN and Graphoormer-GD~\citep{anonymous2023rethinking}.


