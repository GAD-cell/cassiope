


% \begin{table*}[h]
% \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
%     \hline
%         & Actor & Computers & Photo & Chameleon & CiteSeer & Cora & Cornell & PubMed & Squirrel & Wisconsin \\ \hline
%         LVCGNN & 37.3\sd{0.5} & \textbf{90.7} & \textbf{96.1}\sd{0.2} & \textbf{72.8}\sd{0.2} & 78.0\sd{1.0} & \textbf{88.7}\sd{0.1} & 89.5\sd{2.7} & \textbf{90.2}\sd{3.5} & \textbf{59.0}\sd{0.3} & 91.2\sd{1.0} \\ \hline
%         GPRGNN & 39.3\sd{0.3} & 82.9\sd{0.4}(FIXME) & 91.9\sd{0.3}(FIXME) & 67.5\sd{0.4} & 67.6\sd{0.4}(FIXME) & 79.5\sd{0.4}(FIXME) & 91.4\sd{0.7} & 85.1\sd{0.1}(FIXME) & 49.9\sd{0.5} & -\\ \hline
%         BernNet & 41.79 & 87.6 & 93.6 & 68.3 & 80.1 & 88.5 & 92.1 & 88.5 & 51.4 & -\\ \hline
%         $H_2GCN$ & 35.9\sd{1.0} & - & - & 59.4\sd{2.0} & 77.1\sd{1.6} & 87.8\sd{1.4} & 82.2\sd{4.8} & 89.6\sd{0.3} & 37.9\sd{2.0} & 86.7.3\sd{4.7} \\ \hline
%         GCN+JK & 34.2\sd{0.9} & - & - & 63.4\sd{2.0} & 74.5\sd{1.8} & 85.8\sd{0.9} & 64.6\sd{8.7} & 88.4\sd{0.5} & 40.5\sd{1.6} & 74.3\sd{6.4} \\ \hline
%     \end{tabular}
%     }
% \end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\section{Experiments}
We evaluate \model{} on two prediction tasks: vertex classification and graph classification.
\model{} is implemented using Pytorch and Pytorch Geometric~\citep{fey2019fast}. Experiments are run on a single NVIDIA RTX A6000 GPU with 48GB memory.

\begin{table}[t]
\centering
\caption{Vertex classification results on homophilic datasets. }
\label{tab:vertexclassificationhomo}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
         & \amazoncomputers{} & \amazonphoto{} & \citeseer{} & \cora{} & \pubmed{} \\ \midrule
        MLP & 82.9\sd{0.4} & 84.7\sd{0.3} & 76.6\sd{0.9} & 77.0\sd{1.0} & 85.9\sd{0.2} \\
        GCN & 83.3\sd{0.3} & 88.3\sd{0.7} & 79.9\sd{0.7} & 87.1\sd{1.0} & 86.7\sd{0.3} \\
        $\text{GCN+JK}^{\dagger}$ & - & - & 74.5\sd{1.8} & 85.8\sd{0.9} & 88.4\sd{0.5} \\ 
        GAT & 83.3\sd{0.4} & 90.9\sd{0.7} & \textbf{80.5}\sd{0.7} & 88.0\sd{0.8} & 87.0\sd{0.2} \\
        APPNP & 85.3\sd{0.4} & 88.5\sd{0.3} & 80.5\sd{0.7} & 88.1\sd{0.7} & 88.1\sd{0.3} \\
        ChevNet & 87.5\sd{0.4} & 93.8\sd{0.3} & 79.1\sd{0.8} & 86.7\sd{0.8} & 88.0\sd{0.3} \\
        GPRGNN & 86.9\sd{0.3} & 93.9\sd{0.3} & 80.1\sd{0.8} & 88.6\sd{0.7} & 88.5\sd{0.3} \\ 
        BernNet & 87.6\sd{0.4} & 93.6\sd{0.4} & 80.1\sd{0.8} & 88.5 & 88.5\sd{1.0} \\ 
        $\text{H}_2\text{GCN}^{\dagger}$ & - & - & 77.1\sd{1.6} & 87.8\sd{1.4} & 89.6\sd{0.3} \\\midrule 
        \model{}-BF & 90.7 & \textbf{96.1}\sd{0.2} & 78.0\sd{1.0} & 88.7\sd{0.1} & \textbf{90.2}\sd{3.5} \\
        \model{}-DF & \textbf{90.9}\sd{0.4} & 95.2\sd{0.8}& 79.7\sd{0.7} & \textbf{89.5}\sd{0.6} & 89.5\sd{0.6}\\ \bottomrule
    \end{tabular}
    }
%\end{table}
%\begin{table}[t]
%\centering
\vspace{1em}

\caption{Vertex classification results on heterophilic datasets.}
\label{tab:vertexclassificationhete}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
         & \wisconsin{} & \cornell{} & \texas{} & \chameleon{} & \squirrel{} \\ \midrule
        MLP & 85.3\sd{3.6} & 90.8\sd{1.6} & 91.5\sd{1.1} & 46.9\sd{1.5} & 31.0\sd{1.2} \\
        GCN & 59.8\sd{7.0}& 65.9\sd{4.4} & 77.4\sd{3.3} & 59.6\sd{2.2} & 46.8\sd{0.9} \\
        $\text{GCN+JK}^{\dagger}$ & 74.3\sd{6.4} & 74.3\sd{6.4} & 64.6\sd{8.7} & 63.4\sd{2.0}& 40.5\sd{1.6} \\ 
        GAT & 55.3\sd{8.7} & 78.2\sd{3.0} & 80.8\sd{2.1} & 63.1\sd{1.9} & 44.5\sd{0.9} \\
        APPNP & - & \textbf{91.8}\sd{2.0} & 91.0\sd{1.6} & 51.8\sd{1.8} & 34.7\sd{0.6} \\
        ChevNet & 82.6\sd{4.6} & 83.9\sd{2.1} & 86.2\sd{2.5} & 59.3\sd{1.3} & 40.6\sd{0.4} \\
        GPRGNN & - & 91.4\sd{1.8} & 93.0\sd{1.3} & 67.3\sd{1.1} & 50.2\sd{1.9} \\ 
        $\text{H}_2\text{GCN}^{\dagger}$ & 86.7\sd{4.7}& 82.2\sd{4.8} & 84.5\sd{6.8} & 59.4\sd{2.0} & 37.9\sd{2.0} \\ \midrule
        \model{}-BF & \textbf{91.2}\sd{1.0} & 89.5\sd{2.7} & 88.7\sd{4.3} & \textbf{72.8}\sd{0.2} & \textbf{59.0}\sd{0.3} \\
        \model{}-DF & 84.1\sd{3.6} &  83.2\sd{5.8} & 86.8\sd{5.2} & 56.6\sd{3.0} & 47.0\sd{1.5}\\ \bottomrule
    \end{tabular}
    }
\end{table}


\subsection{Vertex Classification}

\paragraph{Datasets.} 
We use three citation graphs, \cora{}, \citeseer{} and \pubmed{}, and two Amazon co-purchase graphs, \amazoncomputers{} and \amazonphoto{}. As shown in \citet{chien21} these five datasets are homophilic graphs on which adjacent vertices tend to share the same label. We also use two Wikipedia graphs, \chameleon{} and \squirrel{}, and two webpage graphs, \texas{} and \cornell{}, from WebKB~\citep{Pei2020}. These five datasets are heterophilic datasets on which adjacent vertices tend to have different labels.
Details about these datasets are shown in \cref{tab:statistics_vertex_classification}.




\paragraph{Setup and baselines.} We adopt the same experimental setup as \citet{bernnet21}, where each dataset is randomly split into train/validation/test set with the ratio of 60\%/20\%/20\%. In total, we use 10 random splits for each dataset, and the reported results are averaged over all splits.

We compare \model{} with seven baseline models: MLP, GCN~\citep{kipf2016semi}, GAT~\citep{velivckovic2017graph}, APPNP~\citep{KlicperaBG19}, ChebNet~\cite{defferrard2016convolutional}, GPRGNN~\citep{chien21}, and BernNet~\cite{bernnet21}.

We perform a hyperparameter search on four parameters in the following ranges:
$\text{number of layers}\in \{1, 2,3,4,5\}$,
$\text{dropout probability} \in \{0.2,0.5,0.7,0.9\}$,
$\delta\in\{1,2,3,4\}$, and
$\text{hidden layer dimension}\in\{64,128\}$.

\paragraph{Observation.}
Results on homophilic and heterophilic datasets are presented in \cref{tab:vertexclassificationhomo,tab:vertexclassificationhete}, respectively. Results marked with $\dagger$ are obtained from \citet{zhu2020beyond}, and other baseline results are taken from \citet{bernnet21}.

From \cref{tab:vertexclassificationhomo,tab:vertexclassificationhete}, we can see that \model{} generalizes to both homophilic and heterophilic graphs in vertex classification tasks. Specifically, \model{}-BF outperforms baselines in 7 out of 10 datasets, while \model{}-DF outperforms 3 out of 10. \model{}-BF performs better than \model{}-DF in heterophilic graphs. We find that the number of vertices in $N_{\delta}(v)$ grows much faster for \model{}-DF than \model{}-BF as $\delta$ increases. This can be explained as the paths to reach each $u\in N_{\delta}(v)$ from $v$ are longer in DFS than that of BFS (in BFS the path lengths are always less than or equal to $\delta$). Therefore more vertices are included in $N_{\delta}(v)$ for DFS. This further implies more vertex features are aggregated for each vertex in \model{}-DF, resulting in over-squashing which degrades the performance of \model{}-DF on heterophilic graphs.

We also list the training runtime in \cref{tab:runtime}. As expected, as $\delta$ increases, the runtime of \model{}-BF increases but stays on par with GCN. When $\delta=1$, \model{}-DF aggregates more vertices thus is slower than \model{}-BF

\begin{table}[ht]
    \centering
    \caption{Training runtime per epoch in seconds.}
    \label{tab:runtime}
    \resizebox{0.9\columnwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
         & \shortstack{GCN\\ \phantom{} } & \shortstack{SGN-BF\\($\delta=1$)}& \shortstack{SGN-BF\\($\delta=2$)} & \shortstack{SGN-BF\\($\delta=3$)} & \shortstack{SGN-DF\\($\delta=1$)} \\
         \midrule
         \cora{} & 0.177 & 0.125 & 0.213 & 0.239 & 0.179 \\
         \pubmed{} & 0.349 & 0.224 & 0.315 & 1.271 & 0.219 \\
         \chameleon{} & 0.205 & 0.198 & 0.457 & - & 0.346 \\
         \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Graph Classification}
\paragraph{Datasets.} We evaluate \model{} on graph classification for chemical compounds, using four molecular datasets: \dd{}~\citep{Dobson2003-dd}, \proteins{}~\citep{Borgwardt2005-proteins}, \nci{}~\citep{Wale2008-nci1} and \enzymes{}~\citep{Schomburg2004-enzymes}. We also include a social dataset \imdbb{}. Following \citet{errica2019fair}, for molecular datasets, vertex features are a one-hot encoding of atom type, with the exception of \enzymes{} where additional 18 features are used, whereas for \imdbb{} the degree of each vertex is the sole vertex feature.
Details about these datasets are shown in \cref{tab:statistics_graph_classification}.

\paragraph{Setup and baselines.} 
We adopt the fair and reproducible evaluation setup from \citet{errica2019fair}, which uses an internal hold-out model selection for each of the 10-fold cross-validation stratified splits. 

We compare \model{} against five GNNs: DGCNN~\citep{zhang2018end}, DiffPool~\citep{Ying18diffpool}, ECC~\citep{Simonovsky17ECC}, GIN~\citep{xu2018powerful} and GraphSAGE~\cite{hamilton2017inductive}, as well as two variants of the contextual graph Markov model: E-CGMM~\citep{Atzeni21E-CGMM} and ICGMM~\citep{Castellana22ICGMM}. We also include a competitive structure-agnostic baseline method, dubbed \baseline{}, from \citet{errica2019fair}.

We perform the hyperparameter search:
$\text{number of layers}\in \{1, 2,3,4,5\}$,
$\text{dropout probability} \in \{0.2, 0.5, 0.7\}$,
$\delta\in\{1,2,3\}$, and
$\text{hidden layer dimension}\in\{64\}$.


\paragraph{Observation.} 
Results are presented in \cref{tab:graphclassification}. Results marked with $\ddagger$ are obtained from \citet{Castellana22ICGMM}, and other baseline results are taken from \citet{errica2019fair}.

We first observe that \model{}-DF outperforms other GNNs and CGMM variants consistently. \model{}-DF also outperforms \baseline{} on 4 out of 5 datasets. Although \model{}-BF also yields competitive results on several benchmarks, it does not perform better than \model{}-DF. This suggests that the graph properties captured by \model{}-DF, such as biconnectivity, might be useful to classify such graphs. 

\begin{table}[t]
\centering
\caption{Graph classification results.}
\label{tab:graphclassification}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
         & \dd{} & \nci{} & \proteins{} & \enzymes{} & \imdbb{}\\ \midrule
        \baseline{} & \textbf{78.4}\sd{4.5} & 69.8\sd{2.2} & 75.8\sd{3.7} & 65.2\sd{6.4} & 70.8\sd{5.0}\\
        DGCNN & 76.6\sd{4.3} & 76.4\sd{1.7} & 72.9\sd{3.5} & 38.9\sd{5.7} & 69.2\sd{3.0}\\
        DiffPool & 75.0\sd{3.5} & 76.9\sd{1.9} & 73.7\sd{3.5} & 59.5\sd{5.6} & 68.4\sd{3.3}\\
        ECC & 72.6\sd{4.1} & 76.2\sd{1.4} & 72.3\sd{3.4} & 29.5\sd{8.2} & 67.7\sd{2.8}\\
        GIN & 75.3\sd{2.9} & 80.0\sd{1.4} & 73.3\sd{4.0} & 59.6\sd{4.5} & 71.2\sd{3.9}\\
        GraphSAGE & 72.9\sd{2.0} & 76.0\sd{1.8} & 73.0\sd{4.5} & 58.2\sd{6.0} & 68.8\sd{4.5}\\
        E-CGMM\textsuperscript{$\ddagger$} & 73.9\sd{4.1} & 78.5\sd{1.7} & 73.3\sd{4.1} & - & 70.7\sd{3.8} \\
        ICGMM\textsuperscript{$\ddagger$} & 76.3\sd{5.6} & 77.6\sd{1.5} & 73.3\sd{2.9} & - & 73.0\sd{4.3}\\
        \midrule
        \model{}-BF & 76.3\sd{3.2} & 78.8\sd{2.9} & 74.0\sd{3.9} & 64.8\sd{7.2} & 71.4\sd{7.1}\\
        \model{}-DF & 78.01\sd{4.0} & \textbf{81.0}\sd{1.4} & \textbf{76.1}\sd{1.6} & \textbf{66.9}\sd{7.5} & \textbf{72.3}\sd{5.4}\\ \bottomrule
    \end{tabular}
    }
\end{table}

% \begin{table}[h!]
%     \centering
%     \caption{Statistics of datasets used for vertex classification.}
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lcccccccccc}
%         \midrule
%          & \cora{} & \citeseer{} & \pubmed{} & \amazoncomputers{} & \amazonphoto{} & \chameleon{} & \squirrel{} & \texas{} & \cornell{} & \wisconsin{} \\
%          \midrule
%          \# Vertices & 2708 & 3327 & 19717 & 13752 & 7650 & 2277 & 5201 & 183 &  183 & 251 \\
%          \# Edges & 5278 & 4552 & 44324 & 245861 & 119081 & 31371 & 198353 & 279 &  277 & 466 \\
%          \# Features & 1433 & 3703 & 500 & 767 & 745 & 2325 & 2089 & 1703 & 1703 & 1703\\
%          \# Classes & 7 & 6 & 5 & 10 & 8 & 5 & 5 & 5 & 5 & 5\\
%          \bottomrule
%     \end{tabular}
%     }
% \end{table}




% \begin{table}[ht]
% \centering
%     \resizebox{0.6\columnwidth}{!}{
%     \begin{tabular}{|c|c|c|}
%     \hline
%         & MUTAG & PTC-MR \\ \hline
%         LVCGNN & 92.9 & 66.7 \\ \hline
%         GIN & 89.4 & 64.6 \\ \hline
%     \end{tabular}
%     }
% \end{table}
