\section{Introduction}
Graph neural networks (GNNs) have emerged as the de-facto method for representation learning on graphs. One popular architecture of GNNs is the message-passing neural networks (MPNNs) which propagate information between vertices along edges~\citep{gilmer2017neural, kipf2016semi,velivckovic2017graph}.  In~\citet{xu2018powerful}, it is shown that the design of MPNNs aligns with the Weisfeiler-Lehman (1-WL) test, a classical algorithm for testing graph isomorphism. Therefore, the expressivity of MPNNs is upper-bounded by the 1-WL test. Intuitively, if two vertices have the same computational/message-passing graph in an MPNN, they are indistinguishable.

%On the one hand, 
Recent studies attempted to increase the expressivity of GNNs beyond the 1-WL. One direction is to extend GNNs to match higher-order WL tests~\citep{morris2019weisfeiler, morris2020weisfeiler, maron2019provably, geerts2022expressiveness}. While these methods offer improved expressivity, they come at the cost of decreased efficiency, as higher-order WL tests are known to be computationally expensive.
%both computationally and memory intensive. 
Another line of research focuses on incorporating graph substructures into feature aggregation~\citep{bodnar2021weisfeiler, bodnar2021weisfeilercellular}. However, these approaches often rely on task-specific, hand-picked substructures. One other strategy is to enhance vertex or edge features with additional distance information relative to target vertices~\citep{YouYL19positionaware,LiWWL20distanceencode}.
%On the other hand, \citet{Velickovic_neuralexecutoin} show MPNN can imitate classical graph algorithms to learn shortest path (Bellman-Ford algorithm) and minimum spanning tree (Primâ€™s algorithm). \citet{dobrik_neuralbipartite} shows MPNN can execute the more complex Ford-Fulkerso algorithm, which consists of several composable subroutines, for finding maximum flow. \citet{xu20_gnnreasoning} analyses MPNN from the perspective of algorithmic alignment: a model can learn a reasoning task well if its computation structure aligns well with the algorithmic structure of the relevant reasoning process. They show MPNN aligns well with dynamic programming so it is theoretically capable to solve these tasks. However, \citet{anonymous2023rethinking} show MPNN cannot solve the biconnectivity problem which can be efficiently solved using depth-first search algorithm.
Despite the efforts, \citet{anonymous2023rethinking} have shown that MPNNs cannot solve the biconnectivity problem, which can be efficiently solved using the depth-first search algorithm~\citep{Tarjan1974-eb}. %, \q{despite their effort (whose effort does it refer to?)}.

In light of the aforementioned understanding, one may question how the design of GNNs can surpass the limitations of 1-WL to address issues that cannot be resolved by MPNNs. %\q{(this sentence reads strange - need some revision)}.
In this work, we systematically study an alternative approach to MPNN, which propagates information along graph search trees. This paper makes the following contributions:
\begin{itemize}
    \item We design a novel colouring scheme, called \emph{local vertex colouring} (LVC), based on breath-first and depth-first search algorithms, that goes beyond 1-WL.
    \item We show that LVC can learn representations to distinguish several graph properties such as biconnectivity, cycles, cut vertices and edges, and \textit{ego short-path graphs} (ESPGs) that 1-WL and MPNNs cannot.
    \item We analyse the expressivity of LVC in terms of  \emph{breadth-first colouring} and \emph{depth-first colouring}, and provide systematical comparisons with 1-WL and 3-WL. 
    \item We further design a graph search-guided GNN architecture, \textit{Search-guided Graph Neural Network}~(\model{}), which inherits the properties of LVC.
\end{itemize}
