Topic,Count,Name,Representation,Representative_Docs
-1,764,-1_learning_model_data_training,"['learning', 'model', 'data', 'training', 'models', 'neural', 'using', 'set', 'figure', 'networks']","['Zeroth-Order Non-Convex Learning via Hierarchical Dual Averaging', 'Growing Action Spaces', 'Greedy-based Value Representation for Optimal Coordination in Multi-agent Reinforcement Learning', 'GraphOpt Learning Optimization Models of Graph Formation', 'GradientDICE Rethinking Generalized Offline Estimation of Stationary Values']"
0,857,0_policy_learning_reward_state,"['policy', 'learning', 'reward', 'state', 'agent', 'algorithm', 'reinforcement', 'regret', 'action', 'function']","['Maximum Mean Discrepancy Test is Aware of Adversarial Attacks', 'Evaluating the Adversarial Robustness of Adaptive Test-time Defenses', 'Adversarial Robustness Against the Union of Multiple Perturbation Models', 'Adversarial Robustness for Code', 'Adversarial Vulnerability of Randomized Ensembles']"
1,197,1_adversarial_attacks_robustness_attack,"['adversarial', 'attacks', 'robustness', 'attack', 'robust', 'training', 'examples', 'accuracy', 'models', 'data']","['Width Provably Matters in Optimization for Deep Linear Neural Networks', 'On the Impact of the Activation Function on Deep Neural Networks Training', 'Gradient Descent Finds Global Minima of Deep Neural Networks', 'How Powerful are Shallow Neural Networks with Bandlimited Random Weights', 'Implicit Regularization with Polynomial Growth in Deep Tensor Factorization']"
2,116,2_networks_neural_network_deep,"['networks', 'neural', 'network', 'deep', 'training', 'loss', 'initialization', 'layer', 'gradient', 'lemma']","['Optimization of Graph Neural Networks Implicit Acceleration by Skip Connections and More Depth', 'A New Perspective on the Effects of Spectrum in Graph Neural Networks', 'Training Graph Neural Networks with 1000 Layers', 'Bayesian Graph Neural Networks with Adaptive Connection Sampling', ""Let's Agree to Degree Comparing Graph Convolutional Networks in the Message-Passing Framework""]"
3,102,3_graph_graphs_node_nodes,"['graph', 'graphs', 'node', 'nodes', 'gnns', 'networks', 'gnn', 'neural', 'gcn', 'learning']","['Variance Reduction via Primal-Dual Accelerated Dual Averaging for Nonsmooth Convex Finite-Sums', 'On a Combination of Alternating Minimization and Nesterovs Momentum', 'Coordinate Descent Methods for Fractional Minimization', 'Cyclic Block Coordinate Descent With Variance Reduction for Composite Nonconvex Optimization', 'Estimate Sequences for Variance-Reduced Stochastic Composite Optimization']"
4,95,4_convex_convergence_gradient_algorithm,"['convex', 'convergence', 'gradient', 'algorithm', 'stochastic', 'optimization', 'accelerated', 'method', 'methods', 'nonconvex']","['Label Distributionally Robust Losses for Multi-class Classification Consistency, Robustness and Adaptivity', 'From Noisy Prediction to True Label Noisy Prediction Calibration via Generative Model', 'Efficient PAC Learning from the Crowd with Pairwise Comparisons', 'Error-Bounded Correction of Noisy Labels', 'Estimating Instance-dependent Bayes-label Transition Matrix using a Deep Neural Network']"
5,88,5_label_labels_noisy_loss,"['label', 'labels', 'noisy', 'loss', 'noise', 'learning', 'classication', 'data', 'risk', 'class']","['A Neural Tangent Kernel Perspective of GANs', 'Implicit competitive regularization in GANs', 'Non-Parametric Priors For Generative Adversarial Networks', 'Augmented CycleGAN Learning Many-to-Many Mappings from Unpaired Data', 'Multilinear Latent Conditioning for Generating Unseen Attribute Combinations']"
6,81,6_gan_gans_discriminator_generator,"['gan', 'gans', 'discriminator', 'generator', 'generative', 'images', 'training', 'adversarial', 'image', '2017']","['Understanding Clipping for Federated Learning Convergence and Client-Level Differential Privacy', 'New Oracle-Efficient Algorithms for Private Synthetic Data Release', 'Locally Private Hypothesis Testing', 'Leveraging Public Data for Practical Private Query Release', 'Learning-augmented private algorithms for multiple quantile release']"
7,80,7_privacy_private_dp_mechanism,"['privacy', 'private', 'dp', 'mechanism', 'data', 'algorithm', 'differentially', 'differential', 'theorem', 'accuracy']","['A Reductions Approach to Fair Classification', 'Model-Agnostic Characterization of Fairness Trade-offs', 'Understanding Instance-Level Impact of Fairness Constraints', 'Two Simple Ways to Learn Individual Fairness Metrics from Data', 'Testing Group Fairness via Optimal Transport Projections']"
8,56,8_fairness_fair_group_sensitive,"['fairness', 'fair', 'group', 'sensitive', 'groups', 'data', 'attributes', 'classier', 'learning', 'classication']","['XAI for Transformers Better Explanations through Conservative Propagation', 'Transformers are RNNs Fast Autoregressive Transformers with Linear Attention', 'SparseBERT Rethinking the Importance Analysis in Self-attention', 'Which transformer architecture fits my data A vocabulary bottleneck in self-attention', 'Low-Rank Bottleneck in Multi-head Attention Models']"
9,55,9_attention_transformer_transformers_selfattention,"['attention', 'transformer', 'transformers', 'selfattention', 'language', 'layer', 'models', 'model', '2021', 'head']","['Median Matrix Completion from Embarrassment to Optimality', 'Schatten Norms in Matrix Streams Hello Sparsity Goodbye Dimension', 'Input-Sparsity Low Rank Approximation in Schatten Norm', 'Generalized Leverage Scores Geometric Interpretation and Applications', 'Leveraging Well-Conditioned Bases Streaming and Distributed Summaries in Minkowski p-Norms']"
10,52,10_matrix_matrices_algorithm_lemma,"['matrix', 'matrices', 'algorithm', 'lemma', 'regression', 'theorem', 'rank', 'problem', 'lasso', 'probability']","['Continuously Indexed Domain Adaptation', 'On Balancing Bias and Variance in Unsupervised Multi-Source-Free Domain Adaptation', 'Understanding Self-Training for Gradual Domain Adaptation', 'Understanding Gradual Domain Adaptation Improved Analysis, Optimal Path and Beyond', 'Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation']"
11,52,11_domain_target_adaptation_source,"['domain', 'target', 'adaptation', 'source', 'shift', 'domains', 'distribution', 'data', 'spurious', 'training']","['Bayesian Algorithm Execution Estimating Computable Properties of Black-box Functions Using Mutual Information', 'Optimization fast and slow optimally switching between local and Bayesian optimization', 'Bayesian Optimization of Composite Functions', 'Multi-fidelity Bayesian Optimization with Max-value Entropy Search', 'Multi-objective Bayesian Optimization using Pareto-frontier Entropy']"
12,50,12_optimization_bayesian_function_design,"['optimization', 'bayesian', 'function', 'design', 'acquisition', 'bo', 'functions', 'objective', 'gp', 'problem']","['Validating Causal Inference Methods', 'On the Identifiability and Estimation of Causal Location-Scale Noise Models', 'Causal Inference using Gaussian Processes with Structured Latent Confounders', 'Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding', 'Classifying Treatment Responders Under Causal Effect Monotonicity']"
13,48,13_causal_treatment_counterfactual_data,"['causal', 'treatment', 'counterfactual', 'data', 'variables', 'effect', 'intervention', 'interventions', 'observational', 'confounding']","['Towards More Efficient Stochastic Decentralized Learning Faster Convergence and Sparse Communication', 'On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization', 'D2 Decentralized Training over Decentralized Data', 'Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication', 'Detached Error Feedback for Distributed SGD with Random Sparsification']"
14,47,14_communication_distributed_sgd_local,"['communication', 'distributed', 'sgd', 'local', 'gradient', 'decentralized', 'compression', 'nnx', 'stochastic', 'convergence']","['Universal Equivariant Multilayer Perceptrons', ""Brauer's Group Equivariant Neural Networks"", 'Equivariance with Learned Canonicalization Functions', 'Equivariant Networks for Pixelized Spheres', 'Provably Strict Generalisation Benefit for Equivariant Models']"
15,46,15_equivariant_group_3d_equivariance,"['equivariant', 'group', '3d', 'equivariance', 'convolution', 'networks', 'groups', 'rotation', 'image', 'point']","['Mirror Sinkhorn Fast Online Optimization on Transport Polytopes', 'Discrete Probabilistic Inverse Optimal Transport', 'Unbalanced minibatch Optimal Transport; applications to Domain Adaptation', 'On Transportation of Mini-batches A Hierarchical Approach', 'On Unbalanced Optimal Transport An Analysis of Sinkhorn Algorithm']"
16,46,16_transport_ot_optimal_wasserstein,"['transport', 'ot', 'optimal', 'wasserstein', 'distance', 'sinkhorn', 'barycenter', 'problem', 'algorithm', 'measures']","['Semi-Implicit Variational Inference', 'Semi-Amortized Variational Autoencoders', 'On the Difficulty of Unbiased Alpha Divergence Minimization', 'Variational Laplace Autoencoders', 'Meta-Learning with Shared Amortized Variational Inference']"
17,44,17_variational_inference_posterior_gradient,"['variational', 'inference', 'posterior', 'gradient', 'distribution', 'ae', 'elbo', 'estimator', 'iwae', 'mcmc']","['YourTTS Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone', 'Self-supervised Learning with Random-projection Quantizer for Speech Recognition', 'A3T Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing', 'Almost Unsupervised Text to Speech and Automatic Speech Recognition', 'Fitting New Speakers Based on a Short Untranscribed Sample']"
18,44,18_speech_audio_speaker_model,"['speech', 'audio', 'speaker', 'model', 'tts', 'models', 'speakers', 'text', 'voice', 'acoustic']","['A Langevin-like Sampler for Discrete Distributions', 'Amortized Monte Carlo Integration', 'The Boomerang Sampler', 'LSB Local Self-Balancing MCMC in Discrete Spaces', 'DG-LMC A Turn-key and Scalable Synchronous Distributed MCMC Algorithm']"
19,40,19_posterior_mcmc_bayesian_likelihood,"['posterior', 'mcmc', 'bayesian', 'likelihood', 'sampling', 'carlo', 'monte', 'prior', 'distribution', 'langevin']","['Transformation Autoregressive Networks', 'On Energy-Based Models with Overparametrized Shallow Neural Networks', 'Autoregressive Energy Machines', 'Cutting out the Middle-Man Training and Evaluating Energy-Based Models without Sampling', 'Energy-Based Processes for Exchangeable Data']"
20,40,20_density_normalizing_ows_flows,"['density', 'normalizing', 'ows', 'flows', 'generative', 'models', 'data', 'distribution', 'ow', 'samples']","['Variational Nearest Neighbor Gaussian Processes', 'Scalable Gaussian Processes with Grid-Structured Eigenfunctions GP-GRIEF', 'Bias-Free Scalable Gaussian Processes via Randomized Truncations', 'Efficient Transformed Gaussian Processes for Non-Stationary Dependent Multi-class Classification', 'Inter-domain Deep Gaussian Processes']"
21,39,21_gaussian_gp_inducing_kernel,"['gaussian', 'gp', 'inducing', 'kernel', 'processes', 'points', 'inference', 'process', 'gps', 'kernels']","['Accurate Uncertainties for Deep Learning Using Calibrated Regression', 'Amortized Conditional Normalized Maximum Likelihood', 'Deep Probability Estimation', 'Discriminative Jackknife Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions', 'Efficient Evaluation-Time Uncertainty Estimation by Improved Distillation']"
22,39,22_uncertainty_bayesian_posterior_deep,"['uncertainty', 'bayesian', 'posterior', 'deep', 'neural', 'model', 'networks', 'data', 'inference', 'variational']","['Generating Counterfactual Explanations with Natural Language', 'On the Relationship Between Explanation and Prediction A Causal View', 'Explainability as statistical inference', 'Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations', 'Label-Free Explainability for Unsupervised Models']"
23,35,23_maximal_improvement_explanations_shapley,"['maximal', 'improvement', 'explanations', 'shapley', 'explanation', 'ig', 'mnlp', 'feature', 'saliency', 'model']","['Equivariant Priors for Compressed Sensing with Unknown Orientation', 'Prior Image-Constrained Reconstruction using Style-Based Generative Models', 'Instance-Optimal Compressed Sensing via Posterior Sampling', 'Near-Exact Recovery for Tomographic Inverse Problems via Deep Learning', 'Improving Robustness of Deep-Learning-Based Image Reconstruction']"
24,35,24_image_sensing_measurements_reconstruction,"['image', 'sensing', 'measurements', 'reconstruction', 'denoising', 'images', 'generative', 'compressed', 'signal', 'inverse']","['Personalized Federated Learning using Hypernetworks', 'Personalized Federated Learning through Local Memorization', 'Anchor Sampling for Federated Learning with Partial Client Participation', 'Orchestra Unsupervised Federated Learning via Globally Consistent Clustering', 'Architecture Agnostic Federated Learning for Neural Networks']"
25,34,25_federated_clients_local_client,"['federated', 'clients', 'local', 'client', 'fl', 'learning', 'global', 'communication', 'fedavg', 'server']","['Regularized Submodular Maximization at Scale', 'Streaming Submodular Maximization under a k-Set System Constraint', 'Deletion Robust Submodular Maximization over Matroids', 'Fast Maximization of Non-Submodular Monotonic Functions on the Integer Lattice', 'From Sets to Multisets Provable Variational Inference for Probabilistic Integer Submodular Models']"
26,34,26_submodular_algorithm_maximization_elements,"['submodular', 'algorithm', 'maximization', 'elements', 'greedy', 'set', 'function', 'approximation', 'submodularity', 'element']","['A Second look at Exponential and Cosine Step Sizes Simplicity, Adaptivity, and Performance', 'SGD and Hogwild Convergence Without the Bounded Gradients Assumption', 'A Theoretical Analysis of the Learning Dynamics under Class Imbalance', 'Towards Noise-adaptive, Problem-adaptive (Accelerated) Stochastic Gradient Descent', 'Characterization of Convex Objective Functions and Optimal Expected Convergence Rates for SGD']"
27,33,27_sgd_stochastic_gradient_convergence,"['sgd', 'stochastic', 'gradient', 'convergence', 'rate', 'convex', 'step', 'stepsize', 'theorem', 'descent']","['Model-Aware Contrastive Learning Towards Escaping the Dilemmas', 'Self-Damaging Contrastive Learning', 'Global Selection of Contrastive Batches via Optimization on Sample Permutations', 'Do More Negative Samples Necessarily Hurt in Contrastive Learning', 'Perfectly Balanced Improving Transfer and Robustness of Supervised Contrastive Learning']"
28,31,28_contrastive_learning_loss_augmentations,"['contrastive', 'learning', 'loss', 'augmentations', 'rand', '128', 'representation', 'negative', 'supervised', 'augmentation']","['A Free-Energy Principle for Representation Learning', 'PAC Generalization via Invariant Representations', 'Connectivity-Optimized Representation Learning via Persistent Homology', 'Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement', 'Disentangling by Factorising']"
29,31,29_disentanglement_ae_representations_disentangled,"['disentanglement', 'ae', 'representations', 'disentangled', 'latent', 'representation', 'data', 'factors', 'learning', 'topological']","['Few-shot Neural Architecture Search', 'Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search', 'Analyzing and Mitigating Interference in Neural Architecture Search', 'Neural Architecture Search without Training', 'NAS-Bench-101 Towards Reproducible Neural Architecture Search']"
30,31,30_search_architecture_nas_architectures,"['search', 'architecture', 'nas', 'architectures', 'neural', 'space', 'snn', '2019', 'spiking', 'training']","['Individual Calibration with Randomized Forecasting', 'Conformal Prediction Sets with Limited False Positives', 'T-SCI A Two-Stage Conformal Inference Algorithm with Guaranteed Coverage for Cox-MLP', 'Exact Optimization of Conformal Predictors via Incremental and Decremental Learning', 'Modular Conformal Calibration']"
31,31,31_conformal_prediction_calibration_strategic,"['conformal', 'prediction', 'calibration', 'strategic', 'performative', 'coverage', 'expert', 'classication', 'cp', 'distribution']","['Good Subnetworks Provably Exist Pruning via Greedy Forward Selection', 'On the Predictability of Pruning Across Scales', 'Training Your Sparse Neural Network Better with Any Mask', 'A Probabilistic Approach to Neural Network Pruning', 'Why Random Pruning Is All We Need to Start Sparse']"
32,31,32_pruning_sparsity_network_sparse,"['pruning', 'sparsity', 'network', 'sparse', 'networks', 'weights', 'pruned', 'training', 'lottery', 'layer']","['(Individual) Fairness for k-Clustering', 'Coresets for Ordered Weighted Clustering', 'Simple and sharp analysis of k-means', 'Scalable Fair Clustering', 'Bregman Power k-Means for Clustering Exponential Family Data']"
33,30,33_clustering_cluster_algorithm_points,"['clustering', 'cluster', 'algorithm', 'points', 'kmeans', 'centers', 'clusters', 'cost', 'center', 'set']","['EquiBind Geometric Deep Learning for Drug Binding Structure Prediction', 'Hierarchical Generation of Molecular Graphs using Structural Motifs', 'Pocket2Mol Efficient Molecular Sampling Based on 3D Protein Pockets', '3DLinker An E(3) Equivariant Variational Autoencoder for Molecular Linker Design', 'Composing Molecules with Multiple Property Constraints']"
34,30,34_molecular_molecules_molecule_reaction,"['molecular', 'molecules', 'molecule', 'reaction', 'graph', 'chemical', '3d', 'atom', 'generation', 'atoms']","['A Distribution-dependent Analysis of Meta Learning', 'Meta-learning for mixed linear regression', 'A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning', 'A Sample Complexity Separation between Non-Convex and Convex Meta-Learning', 'Provable Guarantees for Gradient-Based Meta-Learning']"
35,29,35_metalearning_task_tasks_learning,"['metalearning', 'task', 'tasks', 'learning', 'maml', 'lemma', 'multitask', 'nnx', 'meta', 'representation']","['On Learning Language-Invariant Representations for Universal Machine Translation', 'Non-Monotonic Sequential Text Generation', 'Analyzing Uncertainty in Neural Machine Translation', 'Text Generation with Diffusion Language Models A Pre-training Approach with Continuous Paragraph Denoise', 'Arithmetic Sampling Parallel Diverse Decoding for Large Language Models']"
36,28,36_translation_language_generation_bleu,"['translation', 'language', 'generation', 'bleu', 'model', 'sequence', 'tokens', 'beam', 'models', 'text']","['Automatic Shortcut Removal for Self-Supervised Representation Learning', 'Multi-Grained Vision Language Pre-Training Aligning Texts with Visual Concepts', 'BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation', 'VLMixer Unpaired Vision-Language Pre-training via Cross-Modal CutMix', 'Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)']"
37,27,37_image_segmentation_object_visual,"['image', 'segmentation', 'object', 'visual', 'clip', 'language', 'images', 'text', 'zeroshot', 'models']","['Differentiable Product Quantization for End-to-End Embedding Compression', 'SDQ Stochastic Differentiable Quantization with Mixed Precision', 'I-BERT Integer-only BERT Quantization', 'Improving Neural Network Quantization without Retraining using Outlier Channel Splitting', 'Overcoming Oscillations in Quantization-Aware Training']"
38,27,38_quantization_compression_quantized_bits,"['quantization', 'compression', 'quantized', 'bits', 'precision', 'bit', 'neural', 'latent', 'coding', 'training']","['Exploring Interpretable LSTM Neural Networks over Multi-Variable Data', 'Deep Latent State Space Models for Time-Series Generation', 'Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time', 'Forecasting Sequential Data using Consistent Koopman Autoencoders', 'STRODE Stochastic Boundary Ordinary Differential Equation']"
39,26,39_time_series_neural_ode,"['time', 'series', 'neural', 'ode', 'forecasting', 'latent', 'model', 'models', 'data', 'system']","['Versatile Verification of Tree Ensembles', 'Learning to branch Generalization guarantees and limits of data-independent discretization', 'Analyzing the tree-layer structure of Deep Forests', 'Decision Trees for Decision-Making under the Predict-then-Optimize Framework', 'Generalized and Scalable Optimal Sparse Decision Trees']"
40,24,40_tree_trees_decision_leaf,"['tree', 'trees', 'decision', 'leaf', 'node', 'depth', '10', 'forests', 'optimal', 'cart']","['Variational Auto-Regressive Gaussian Processes for Continual Learning', 'Overcoming Catastrophic Forgetting by Bayesian Generative Regularization', 'Continual Learning with Guarantees via Weight Interval Constraints', 'Improving Task-free Continual Learning by Distributionally Robust Memory Evolution', 'Kernel Continual Learning']"
41,23,41_memory_continual_forgetting_task,"['memory', 'continual', 'forgetting', 'task', 'tasks', 'learning', 'catastrophic', 'plasticity', 'network', 'neural']","['Which Tricks are Important for Learning to Rank', 'Online Learning to Rank with Features', 'Concentric mixtures of Mallows models for top-k rankings sampling and identifiability', 'Decision-Focused Learning Through the Lens of Learning to Rank', 'GNNRank Learning Global Rankings from Pairwise Comparisons via Directed Graph Neural Networks']"
42,22,42_001000_basketball_ranking_proximal,"['001000', 'basketball', 'ranking', 'proximal', 'dist', 'innerproduct', 'dimpa', '000', 'baseline', '000000']","['Solving high-dimensional parabolic PDEs using the tensor train format', 'Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling', 'Composing Partial Differential Equations with Physics-Aware Neural Networks', 'Implicit Neural Spatial Representations for Time-dependent PDEs', 'Lie Point Symmetry Data Augmentation for Neural PDE Solvers']"
43,21,43_pde_pdes_equation_neural,"['pde', 'pdes', 'equation', 'neural', 'equations', 'pinns', 'model', 'simulation', 'r3', 'graph']","['Do RNN and LSTM have Long Memory', 'Towards Binary-Valued Gates for Robust LSTM Training', 'Dynamical Isometry and a Mean Field Theory of RNNs Gating Enables Signal Propagation in Recurrent Neural Networks', 'Implicit Bias of Linear RNNs', 'Reviving and Improving Recurrent Back-Propagation']"
44,20,44_recurrent_rnn_lstm_gate,"['recurrent', 'rnn', 'lstm', 'gate', 'rnns', 'memory', 'gates', 'hidden', 'neural', 'networks']","['Understanding Failures in Out-of-Distribution Detection with Deep Generative Models', 'Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation', ""Hierarchical VAEs Know What They Don't Know"", 'Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure', 'Latent Outlier Exposure for Anomaly Detection with Contaminated Data']"
45,19,45_detection_ood_anomaly_indistribution,"['detection', 'ood', 'anomaly', 'indistribution', 'outlier', 'outofdistribution', 'data', 'test', '0100', 'training']","['Measuring abstract reasoning in neural networks', 'Learning Reasoning Strategies in End-to-End Differentiable Proving', 'Proving Theorems using Incremental Learning and Hindsight Experience Replay', 'Circuit-Based Intrinsic Methods to Detect Overfitting', 'Neuro-Symbolic Visual Reasoning Disentangling Visual from Reasoning']"
46,18,46_reasoning_task_neural_patt,"['reasoning', 'task', 'neural', 'patt', 'set', 'learning', 'pretraining', 'symbolic', 'model', 'batch04050607080910test']","['Zeno Robust Fully Asynchronous SGD', 'The Hidden Vulnerability of Distributed Learning in Byzantium', 'Asynchronous Byzantine Machine Learning', 'Byzantine Machine Learning Made Easy by Resilient Averaging of Momentums', 'Byzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data']"
47,17,47_byzantine_workers_distributed_gradients,"['byzantine', 'workers', 'distributed', 'gradients', 'attack', 'gradient', 'worker', 'attacks', 'poisoning', 'server']","['Variational Open-Domain Question Answering', 'The Effect of Natural Distribution Shift on Question Answering Models', 'StreamingQA A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models', 'Same Pre-training Loss, Better Downstream Implicit Bias Matters for Language Models', 'REALM Retrieval-Augmented Language Model Pre-Training']"
48,16,48_language_single_prompt_model,"['language', 'single', 'prompt', 'model', 'models', 'questions', 'question', 'retrieval', 'pretraining', 'answer']","['SoQal Selective Oracle Questioning for Consistency Based Active Learning of Cardiac Signals', 'Online Active Regression', 'On the Relationship between Data Efficiency and Error for Uncertainty Sampling', 'Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning', 'Margin-based sampling in high dimensions When being active is less efficient than staying passive']"
49,16,49_active_learning_data_sampling,"['active', 'learning', 'data', 'sampling', 'oracle', 'mal', 'acquisition', 'labels', 'query', 'error']","['Zero-shot Learning and Knowledge Transfer in Music Classification and Tagging', 'XtarNet Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning', 'Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning', 'TapNet Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning', 'Self-Supervised Prototypical Transfer Learning for Few-Shot Classification']"
50,16,50_fewshot_learning_classes_task,"['fewshot', 'learning', 'classes', 'task', 'training', 'miniimagenet', 'tasks', 'classication', 'set', 'bloom']","['Unsupervised Deep Learning by Neighbourhood Discovery', 'Puzzle Mix Exploiting Saliency and Local Statistics for Optimal Mixup', 'Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup', 'From ImageNet to Image Classification Contextualizing Progress on Benchmarks', 'FOCUS Familiar Objects in Common and Uncommon Settings']"
51,15,51_images_imagenet_test_image,"['images', 'imagenet', 'test', 'image', 'accuracy', 'class', 'labels', 'mixup', 'set', 'dataset']","['Video Prediction with Appearance and Motion Conditions', 'Time Is MattEr Temporal Self-supervision for Video Transformers', 'Temporally Consistent Transformers for Video Generation', 'Temporal Gaussian Mixture Layer for Videos', 'Stochastic Video Generation with a Learned Prior']"
52,15,52_video_frames_videos_temporal,"['video', 'frames', 'videos', 'temporal', 'frame', 'motion', 'prediction', 'model', 'image', 'generation']","['Topic Modeling via Full Dependence Mixtures', 'Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network', 'On-the-fly Rectification for Robust Large-Vocabulary Topic Inference', 'Neural Topic Modeling with Continual Lifelong Learning', 'Learning Structured Latent Factors from Dependent Data A Generative Model Framework from Information-Theoretic Perspective']"
53,13,53_topic_word_topics_latent,"['topic', 'word', 'topics', 'latent', 'document', 'words', 'text', 'modeling', 'model', 'embeddings']","['SpreadsheetCoder Formula Prediction from Semi-structured Context', 'Repository-Level Prompt Generation for Large Language Models of Code', 'PAL Program-aided Language Models', 'Open Vocabulary Learning on Source Code with a Graph-Structured Cache', 'Leveraging Language to Learn Program Abstractions and Search Heuristics']"
54,12,54_code_program_prompt_programs,"['code', 'program', 'prompt', 'programs', 'language', 'repair', 'string', 'tmp', 'context', 'model']","['Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel k-means Clustering', 'Random Gegenbauer Features for Scalable Kernel Methods', 'Random Fourier Features for Kernel Ridge Regression Approximation Bounds and Statistical Guarantees', 'Quantization Algorithms for Random Fourier Features', 'One-shot distributed ridge regression in high dimensions']"
55,12,55_kernel_ridge_matrix_kernels,"['kernel', 'ridge', 'matrix', 'kernels', 'regression', 'random', 'theorem', 'features', 'lmrff', 'gaussian']","['Symmetric Spaces for Graph Embeddings A Finsler-Riemannian Approach', 'Representation Tradeoffs for Hyperbolic Embeddings', 'Optimal Transport for structured data with application on graphs', 'On Efficient Low Distortion Ultrametric Embedding', 'Nested Subspace Arrangement for Representation of Relational Data']"
56,12,56_hyperbolic_embeddings_graph_embedding,"['hyperbolic', 'embeddings', 'graph', 'embedding', 'distance', 'graphs', 'disk', 'spaces', 'euclidean', 'space']","['Zero-Shot Knowledge Distillation in Deep Networks', 'Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model', 'Towards Understanding Knowledge Distillation', 'Revisiting Label Smoothing and Knowledge Distillation Compatibility What was Missing', 'Model Ratatouille Recycling Diverse Models for Out-of-Distribution Generalization']"
57,11,57_teacher_distillation_student_kd,"['teacher', 'distillation', 'student', 'kd', 'knowledge', 'transfer', 'domainnet', 'ratatouille', 'teachers', 'training']","['Two-way kernel matrix puncturing towards resource-efficient PCA and spectral clustering', 'Theory of Spectral Method for Union of Subspaces-Based Random Geometry Graph', 'Sublinear-Time Clustering Oracle for Signed Graphs', 'p-Norm Flow Diffusion for Local Graph Clustering', 'Optimal LP Rounding and Linear-Time Approximation Algorithms for Clustering Edge-Colored Hypergraphs']"
58,11,58_clustering_cluster_lp_spectral,"['clustering', 'cluster', 'lp', 'spectral', 'graph', 'subspace', 'signed', 'algorithm', 'edge', 'lemma']","['Zoo-Tuning Adaptive Transfer from a Zoo of Models', 'Failure and success of the spectral bias prediction for Kernel Ridge Regression the case of low-dimensional data', 'Fast Sampling of Diffusion Models via Operator Learning', 'Fast Relative Entropy Coding with A coding', 'Fast rates for noisy interpolation require rethinking the effects of inductive bias']"
